{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/exercise_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeYKb5I5bAeJ"
      },
      "source": [
        "# Task 12: analogy evaluation\n",
        "\n",
        "In the lecture, we touched upon *Mikolov's analogy dataset* which was one of the first analogy evaluation datasets for word embeddings. It consists of 9+5=14 sets of word analogies. You can find it for example here: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt\n",
        "\n",
        "It might be interesting to know how well our embeddings fare on each of these 14 tasks. And that will be our exercise. The steps are as follows:\n",
        "\n",
        "1. Read in the analogy tuples from the file above, for each task separately (the format of the file is kinda self-explanatory once you open it)\n",
        "2. Write a function `eval_analogy(tuples,embeddings,K)` which will return the top-K accuracy of the `embeddings` (Gensim's KeyedVectors) on `tuples`, which are the analogy 4-tuples. For instance for the tuple (\"Athens\",\"Greece\",\"Havana\",\"Cuba\") will be counted as correct if the analogy on the first three words results in K nearest neighbors that contain also \"Cuba\". Hope this is clear. :)\n",
        "3. Run this function on the 14 tasks you read in step (1) and see if you see any interesting differences\n",
        "\n",
        "Below is the relevant embedding-loading and analogy example code from the lecture that you can reuse.\n",
        "\n",
        "**Tip:** these do take a while to compute, so you might want to debug your code on a small sample and when happy, run the whole thing only once. I also like to use `tqdm` to get a progress bar, so I see how long I need to wait to see some output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "c3-FPp5MZjAS"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import os\n",
        "import wget\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRbTPYZGZuiJ",
        "outputId": "626b165a-799c-458c-dd68-d9a1854ceb77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# I found this link in the NLPL repository\n",
        "# It refers to English model trained on the Gigaword corpus of news\n",
        "##!wget http://vectors.nlpl.eu/repository/20/12.zip\n",
        "\n",
        "## Try these if the download above is too slow, I mirrored these:\n",
        "!wget http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
        "#!wget http://dl.turkunlp.org/TKO_7095_2023/42.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "### functions\n",
        "\n",
        "# load file\n",
        "def download_file(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        wget.download(url, filename)\n",
        "    else:\n",
        "        print(f\"{filename} already exists. Skipping download.\")\n",
        "\n",
        "# unzip\n",
        "def unzip_model_bin_files(zip_file):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        # Get a list of all files in the zip archive\n",
        "        file_list = zip_ref.namelist()\n",
        "\n",
        "        # Filter out only the model.bin files\n",
        "        model_bin_files = [file_name for file_name in file_list if os.path.splitext(file_name)[1] == '.bin']\n",
        "\n",
        "        # Extract the model.bin files to the destination folder\n",
        "        for file_name in model_bin_files:\n",
        "            zip_ref.extract(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.zip already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "download_file(\"http://dl.turkunlp.org/TKO_7095_2023/12.zip\", \"12.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unzip_model_bin_files(\"12.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.rename(\"model.bin\", \"en.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL1rXODZOCYi"
      },
      "source": [
        "\n",
        "\n",
        "*   Now we can load the embeddings\n",
        "*   These are huge, but they are sorted by frequency, so we can easily limit ourselves to the top 100,000 words, which will be plenty enough for us\n",
        "*   This is maybe good to note, now we enter the territory of NLP models which count in the gigabytes in size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4DbFt1VOaDCu"
      },
      "outputs": [],
      "source": [
        "# This is how you load the trained embeddings\n",
        "# check the documentation\n",
        "# w2v embeddings are traditionlly distributed in one of two formats: a text form, and a binary form\n",
        "# The embeddings we downloaded above are in the binary form, so we need to indicate that when loading\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_emb_en=KeyedVectors.load_word2vec_format(\"en.bin\", limit=100000, binary=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEsD37OxeP2C"
      },
      "source": [
        "`KeyedVectors` documentation is here: https://radimrehurek.com/gensim/models/keyedvectors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT9CdAlCOW3q"
      },
      "source": [
        "# Basic operations with the embeddings\n",
        "\n",
        "* The KeyedVectors object allows for all the basic operations with embeddings which we saw in the lecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtyrScWuOokr"
      },
      "source": [
        "# Word analogy\n",
        "\n",
        "* \"A is to B as C is to D\"\n",
        "* Can be implemented as D=B-A+C, where (A,B,C) are word embeddings\n",
        "* Then list words nearest to the computed embedding D\n",
        "* In the library, the implementation lets us list words with \"+\" sign, and words with \"-\" sign\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PaVLPxsOujq",
        "outputId": "0ef680e9-6244-46de-d618-ea5f4682430c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Stockholm', 0.7338932752609253),\n",
              " ('Malmo', 0.5458161234855652),\n",
              " ('Helsinki', 0.5444940328598022),\n",
              " ('Goteborg', 0.5421050190925598),\n",
              " ('Swedish', 0.5309098362922668),\n",
              " ('Malmoe', 0.5198634266853333),\n",
              " ('Oslo', 0.5004472732543945),\n",
              " ('Gothenburg', 0.4957912266254425),\n",
              " ('STOCKHOLM', 0.48791587352752686),\n",
              " ('Copenhagen', 0.47769418358802795)]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# B     A      C\n",
        "# Paris-France+Sweden= ___?\n",
        "#\n",
        "# i.e. France is to Paris as Sweden is to X\n",
        "wv_emb_en.most_similar(positive=[\"Paris\",\"Sweden\"],negative=[\"France\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-85ZWiCmPVv9",
        "outputId": "e47ab8bc-12f3-4fd6-da3c-04989d7a6697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cow is to milk as hen is to: sauce\n",
            "Paris is to France as Helsinki is to: Finland\n",
            "car is to wheel as airplane is to: rudder\n",
            "airplane is to propeller as ship is to: vessel\n",
            "king is to queen as man is to: woman\n",
            "man is to doctor as woman is to: physician\n",
            "man is to boss as woman is to: bosses\n"
          ]
        }
      ],
      "source": [
        "triples=[(\"cow\",\"milk\",\"hen\"),\n",
        "         (\"Paris\",\"France\",\"Helsinki\"),\n",
        "         (\"car\",\"wheel\",\"airplane\"),\n",
        "         (\"airplane\",\"propeller\",\"ship\"),\n",
        "         (\"king\",\"queen\",\"man\"),\n",
        "         (\"man\",\"doctor\",\"woman\"),\n",
        "         (\"man\",\"boss\",\"woman\")\n",
        "         ]\n",
        "for what,is_to_what,as_this_is in triples:\n",
        "    # is_to_what-what+as_this_is\n",
        "    to_what=wv_emb_en.most_similar(positive=[is_to_what,as_this_is],negative=[what])[0][0]\n",
        "    print(f\"{what} is to {is_to_what} as {as_this_is} is to: {to_what}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuMn_21DlLTx"
      },
      "source": [
        "# The exercise code starts below\n",
        "\n",
        "* I will donate you a function for reading Mikolov's data, but I recommend you delete it and write your own as a further exercise\n",
        "* Reading annoying file formats is an integral part of NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBx6TuWtliB-",
        "outputId": "2bfff2ae-a098-4e1e-cdbf-b5d0f1ce06f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "questions-words.txt already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "#Remember you always need to download the \"raw\" link from GitHub, or else you get an HTML with the pretty-printed data, not the data itself\n",
        "download_file(\"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\", \"questions-words.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT4EFRMUlwoS",
        "outputId": "df546852-b190-4f38-97ca-27549729a688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 14 tasks.\n"
          ]
        }
      ],
      "source": [
        "tasks={} #A dictionary with taskname as key, and value will then be a list of 4-tuples with the analogy data\n",
        "\n",
        "with open(\"questions-words.txt\") as f:\n",
        "    for line in f:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        if not line:\n",
        "            continue #skip possible empty lines\n",
        "        if line.startswith(\": \"): #All tasks seem to start with a line like \": task-name\"\n",
        "            taskname=line[2:] #get rid of \": \"\n",
        "            tuple_list=[] #let's make a new list for the tuples and store it into the tasks dictionary\n",
        "            #then we keep filling it, until a new task starts, when a new list is created, the previous\n",
        "            #of course remains in the `tasks` dictionary\n",
        "            tasks[taskname]=tuple_list\n",
        "        else: #not a task line, so this should be a 4-word line, with words space-separated it seems\n",
        "            w1,w2,w3,w4=line.split()\n",
        "            tuple_list.append((w1,w2,w3,w4)) #let's append it and we're done\n",
        "\n",
        "print(f\"We have {len(tasks)} tasks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 12: analogy evaluation\n",
        "\n",
        "In the lecture, we touched upon *Mikolov's analogy dataset* which was one of the first analogy evaluation datasets for word embeddings. It consists of 9+5=14 sets of word analogies. You can find it for example here: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt\n",
        "\n",
        "It might be interesting to know how well our embeddings fare on each of these 14 tasks. And that will be our exercise. The steps are as follows:\n",
        "\n",
        "1. Read in the analogy tuples from the file above, for each task separately (the format of the file is kinda self-explanatory once you open it)\n",
        "2. Write a function `eval_analogy(tuples,embeddings,K)` which will return the top-K accuracy of the `embeddings` (Gensim's KeyedVectors) on `tuples`, which are the analogy 4-tuples. For instance for the tuple (\"Athens\",\"Greece\",\"Havana\",\"Cuba\") will be counted as correct if the analogy on the first three words results in K nearest neighbors that contain also \"Cuba\". Hope this is clear. :)\n",
        "3. Run this function on the 14 tasks you read in step (1) and see if you see any interesting differences\n",
        "\n",
        "Below is the relevant embedding-loading and analogy example code from the lecture that you can reuse.\n",
        "\n",
        "**Tip:** these do take a while to compute, so you might want to debug your code on a small sample and when happy, run the whole thing only once. I also like to use `tqdm` to get a progress bar, so I see how long I need to wait to see some output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZkD_Ky24nvqn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task *capital-common-countries* has top-3 accuracy of 93.67588932806325%\n",
            "Task *capital-world* has top-3 accuracy of 87.79840848806366%\n",
            "Task *currency* has top-3 accuracy of 29.676674364896073%\n",
            "Task *city-in-state* has top-3 accuracy of 63.76165383056344%\n",
            "Task *family* has top-3 accuracy of 69.96047430830039%\n",
            "Task *gram1-adjective-to-adverb* has top-3 accuracy of 46.16935483870967%\n",
            "Task *gram2-opposite* has top-3 accuracy of 47.9064039408867%\n",
            "Task *gram3-comparative* has top-3 accuracy of 95.4954954954955%\n",
            "Task *gram4-superlative* has top-3 accuracy of 83.77896613190731%\n",
            "Task *gram5-present-participle* has top-3 accuracy of 78.88257575757575%\n",
            "Task *gram6-nationality-adjective* has top-3 accuracy of 93.1207004377736%\n",
            "Task *gram7-past-tense* has top-3 accuracy of 90.44871794871796%\n",
            "Task *gram8-plural* has top-3 accuracy of 89.03903903903904%\n",
            "Task *gram9-plural-verbs* has top-3 accuracy of 83.44827586206897%\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "\n",
        "\n",
        "def eval_analogy(tuples,embeddings,K):\n",
        "    #### YOUR CODE GOES HERE ########\n",
        "    i = 0\n",
        "    for tuple in tuples:\n",
        "        try:\n",
        "            words = [w for w, value in embeddings.most_similar(positive=[tuple[1], tuple[2]], negative=[tuple[0]], topn=K)]\n",
        "            if tuple[3] in words:\n",
        "                i += 1\n",
        "        except KeyError:\n",
        "            continue    \n",
        "    top_k_acc = (i/len(tuples))*100\n",
        "    return top_k_acc\n",
        "\n",
        "for task in tasks:\n",
        "    print(f\"Task *{task}* has top-3 accuracy of {eval_analogy(tasks[task], wv_emb_en, 3)}%\")\n",
        "    \n",
        "### MY results are\n",
        "# Task *gram9-plural-verbs* has top-3 accuracy of 83.45%\n",
        "# Task *capital-common-countries* has top-3 accuracy of 93.68%\n",
        "# Task *capital-world* has top-3 accuracy of 95.97%\n",
        "# Task *currency* has top-3 accuracy of 40.54%\n",
        "# Task *city-in-state* has top-3 accuracy of 63.76%\n",
        "# Task *family* has top-3 accuracy of 93.16%\n",
        "# Task *gram1-adjective-to-adverb* has top-3 accuracy of 49.25%\n",
        "# Task *gram2-opposite* has top-3 accuracy of 49.62%\n",
        "# Task *gram3-comparative* has top-3 accuracy of 95.50%\n",
        "# Task *gram4-superlative* has top-3 accuracy of 86.32%\n",
        "# Task *gram5-present-participle* has top-3 accuracy of 83.97%\n",
        "# Task *gram6-nationality-adjective* has top-3 accuracy of 95.45%\n",
        "# Task *gram7-past-tense* has top-3 accuracy of 90.45%\n",
        "# Task *gram8-plural* has top-3 accuracy of 89.04%\n",
        "# Task *gram9-plural-verbs* has top-3 accuracy of 83.45%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP67Strdiz3oAGhypUIC5M3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
